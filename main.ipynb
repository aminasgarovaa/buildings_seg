{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/facebookresearch/dinov3.git\n",
    "# pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58266443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "# git clone https://github.com/facebookresearch/Mask2Former.git\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0910c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained backbones\n",
    "import torch\n",
    "\n",
    "DINOv3_REPO = \"/home/aminaasgarova/Desktop/dinov3/dinov3\"\n",
    "DINOv3_weights = \"/home/aminaasgarova/Desktop/dinov3/dinov3/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\"\n",
    "\n",
    "# DINOv3 ViT models pretrained on satellite imagery\n",
    "dinov3_vitl16 = torch.hub.load(DINOv3_REPO, 'dinov3_vitl16', source='local', weights = DINOv3_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "def make_transform(resize_size: int = 256): \n",
    "    to_tensor = v2.ToImage()\n",
    "    resize = v2.Resize((resize_size, resize_size), antialias=True)\n",
    "    to_float = v2.ToDtype(torch.float32, scale=True)\n",
    "    normalize = v2.Normalize(\n",
    "        mean=(0.430, 0.411, 0.296),\n",
    "        std=(0.213, 0.156, 0.143),\n",
    "    )\n",
    "    return v2.Compose([to_tensor, resize, to_float, normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 256, 256)\n",
      "dtype: uint16\n",
      "min: 1\n",
      "max: 255\n"
     ]
    }
   ],
   "source": [
    "# Dataset expolaration\n",
    "import numpy as np\n",
    "\n",
    "x = np.load(\"/home/aminaasgarova/Desktop/dinov33/data/train_images/0a0c3d101a954cb9950f476274bc70b4.npy\")\n",
    "print(\"shape:\", x.shape)\n",
    "print(\"dtype:\", x.dtype)\n",
    "print(\"min:\", x.min())\n",
    "print(\"max:\", x.max())\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SAT_MEAN = (0.430, 0.411, 0.296)\n",
    "SAT_STD  = (0.213, 0.156, 0.143)\n",
    "\n",
    "def dinov3_input_from_npy(npy_path: str):\n",
    "    x = np.load(npy_path)  # your array: (3, H, W), uint16, 0..255\n",
    "\n",
    "    # 1) make float32 in [0, 1]\n",
    "    x = x.astype(np.float32) / 255.0\n",
    "\n",
    "    # 2) to torch (C,H,W)\n",
    "    x = torch.from_numpy(x)  # float32\n",
    "\n",
    "    # 3) normalize\n",
    "    mean = torch.tensor(SAT_MEAN)[:, None, None]\n",
    "    std  = torch.tensor(SAT_STD)[:, None, None]\n",
    "    x = (x - mean) / std\n",
    "\n",
    "    # 4) add batch dim -> (1,3,H,W)\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "inp = dinov3_input_from_npy(\"/mnt/data/0a0c3d101a954cb9950f476274bc70b4.npy\")\n",
    "print(inp.shape, inp.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191f306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e12b61b",
   "metadata": {},
   "source": [
    "## Mask2Former head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ccbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmdetection \n",
    "# conda create -n dinov3_m2f python=3.10 -y\n",
    "# conda activate dinov3_m2f\n",
    "\n",
    "# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "# python -m pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# pip install -U openmim\n",
    "# mim install mmengine\n",
    "# mim install \"mmcv==2.1.0\"\n",
    "# mim install \"mmdet>=3.0.0\"\n",
    "# mim install \"mmsegmentation>=1.2.0\"\n",
    "# pip install rasterio tifffile pillow opencv-python\n",
    "\n",
    "\n",
    "# git clone https://github.com/open-mmlab/mmsegmentation.git\n",
    "# cd mmsegmentation\n",
    "# pip install -e .\n",
    "\n",
    "# mkdir -p projects/dinov3_building\n",
    "# touch projects/__init__.py\n",
    "# touch projects/dinov3_building/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 2 \n",
    "# pip install -U 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "# git clone https://github.com/facebookresearch/Mask2Former.git\n",
    "# cd Mask2Former\n",
    "# pip install -r requirements.txt\n",
    "# export PYTHONPATH=$(pwd):$PYTHONPATH\n",
    "\n",
    "\n",
    "# mkdir -p buildings_project/configs\n",
    "#touch buildings_project/buildings_npy_dataset.py\n",
    "#touch buildings_project/buildings_npy_mapper.py\n",
    "#touch buildings_project/dinov3_backbone.py\n",
    "#touch buildings_project/train_buildings.py\n",
    "#touch buildings_project/infer_one_image.py\n",
    "#touch buildings_project/configs/dinov3_mask2former_buildings.yaml\n",
    "\n",
    "# export PYTHONPATH=/home/aminaasgarova/Desktop/dinov33/Mask2Former:$PYTHONPATH\n",
    "# cd /home/aminaasgarova/Desktop/buildings_project\n",
    "# python train_buildings.py --config-file configs/dinov3_mask2former_buildings.yaml --num-gpus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Option3\n",
    "\n",
    "#1\n",
    "# git clone https://github.com/facebookresearch/dinov3.git\n",
    "# cd dinov3\n",
    "\n",
    "#2\n",
    "# micromamba env create -f conda.yaml\n",
    "# micromamba activate dinov3\n",
    "\n",
    "# conda create -n dinov3 python=3.11.14 -y\n",
    "\n",
    "#3 inside of building_seg_dinov3_mask2former folder \n",
    "# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "# python -m pip install -U pip setuptools wheel\n",
    "# python -m pip install opencv-python pillow rasterio numpy\n",
    "\n",
    "# python -m pip install \"git+https://github.com/facebookresearch/detectron2.git\" --no-build-isolation\n",
    "# git clone https://github.com/facebookresearch/Mask2Former.git\n",
    "# cd Mask2Former\n",
    "# python -m pip install -e .\n",
    "# cd ..\n",
    "\n",
    "# python -m pip install -e /home/aminaasgarova/Desktop/dinov3/dinov3\n",
    "# export PYTHONPATH=/home/aminaasgarova/Desktop/dinov33/dinov3:$PYTHONPATH\n",
    "\n",
    "\n",
    "# Final \n",
    "export PYTHONPATH=/home/aminaasgarova/Desktop/dinov33/dinov3:$PYTHONPATH\n",
    "export PYTHONPATH=$(pwd):$PYTHONPATH\n",
    "python custom/train_net_buildings.py \\\n",
    "  --num-gpus 1 \\\n",
    "  --config-file custom/buildings_mask2former_dinov3.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -L https://micro.mamba.pm/install.sh | bash\n",
    "\n",
    "micromamba remove -y torch torchvision torchaudio\n",
    "\n",
    "micromamba install -y -c pytorch -c nvidia pytorch torchvision torchaudio pytorch-cuda=11.8\n",
    "\n",
    "python -c \"import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.device_count())\"\n",
    "\n",
    "micromamba install -y -c nvidia cuda-nvcc=11.8\n",
    "which nvcc\n",
    "nvcc --version\n",
    "export CUDA_HOME=$CONDA_PREFIXh\n",
    "echo $CUDA_HOME\n",
    "\n",
    "python -m pip uninstall -y detectron2\n",
    "python -m pip install -U \"git+https://github.com/facebookresearch/detectron2.git\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6c1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.40392157, 0.36078432, 0.32156864],\n",
       "        [0.4117647 , 0.35686275, 0.3254902 ],\n",
       "        [0.4862745 , 0.41568628, 0.38431373],\n",
       "        ...,\n",
       "        [0.4745098 , 0.4392157 , 0.40392157],\n",
       "        [0.47058824, 0.43137255, 0.39607844],\n",
       "        [0.4627451 , 0.41960785, 0.38039216]],\n",
       "\n",
       "       [[0.4117647 , 0.3647059 , 0.3254902 ],\n",
       "        [0.45882353, 0.39607844, 0.36078432],\n",
       "        [0.49803922, 0.41568628, 0.3882353 ],\n",
       "        ...,\n",
       "        [0.5058824 , 0.47058824, 0.4392157 ],\n",
       "        [0.45882353, 0.42352942, 0.39215687],\n",
       "        [0.42352942, 0.3882353 , 0.35686275]],\n",
       "\n",
       "       [[0.4392157 , 0.38039216, 0.3372549 ],\n",
       "        [0.46666667, 0.39215687, 0.3529412 ],\n",
       "        [0.4862745 , 0.39215687, 0.3647059 ],\n",
       "        ...,\n",
       "        [0.5294118 , 0.49411765, 0.46666667],\n",
       "        [0.47058824, 0.4392157 , 0.4117647 ],\n",
       "        [0.43529412, 0.40784314, 0.3764706 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.10196079, 0.10196079, 0.08627451],\n",
       "        [0.14901961, 0.14509805, 0.12156863],\n",
       "        [0.21176471, 0.2       , 0.16862746],\n",
       "        ...,\n",
       "        [0.2       , 0.23137255, 0.19607843],\n",
       "        [0.18431373, 0.21960784, 0.18431373],\n",
       "        [0.09019608, 0.13333334, 0.10588235]],\n",
       "\n",
       "       [[0.15686275, 0.16470589, 0.13333334],\n",
       "        [0.17254902, 0.1764706 , 0.14509805],\n",
       "        [0.16470589, 0.16470589, 0.13725491],\n",
       "        ...,\n",
       "        [0.22352941, 0.25882354, 0.21568628],\n",
       "        [0.19607843, 0.23529412, 0.19607843],\n",
       "        [0.10196079, 0.14901961, 0.11372549]],\n",
       "\n",
       "       [[0.22745098, 0.24313726, 0.19607843],\n",
       "        [0.18431373, 0.2       , 0.16078432],\n",
       "        [0.12941177, 0.14117648, 0.10980392],\n",
       "        ...,\n",
       "        [0.24313726, 0.27450982, 0.22745098],\n",
       "        [0.21960784, 0.25882354, 0.21176471],\n",
       "        [0.14901961, 0.19215687, 0.15294118]]],\n",
       "      shape=(256, 256, 3), dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose dataset (C,H.W) as (H, W, C)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_npy_image(path: str):\n",
    "    x = np.load(path)  # could be (C,H,W) or (H,W,C)\n",
    "    if x.ndim == 3 and x.shape[0] in (1, 3, 4):  # (C,H,W)\n",
    "        x = np.transpose(x, (1, 2, 0))          # -> (H,W,C)\n",
    "    x = x.astype(np.float32)\n",
    "    # scale to 0..1 for normalization later\n",
    "    if x.max() > 1.5:\n",
    "        x = x / 255.0 if x.max() <= 255 else x / 65535.0\n",
    "    return x  # (H,W,C) float32\n",
    "\n",
    "def load_npy_mask(path: str):\n",
    "    m = np.load(path)  # (H,W), values 0/1 or 0..K\n",
    "    if m.ndim == 3 and m.shape[0] == 1:\n",
    "        m = m[0]\n",
    "    return m.astype(np.int64)\n",
    "\n",
    "img = \"/home/aminaasgarova/Desktop/dinov33/data/train_images/0a0c3d101a954cb9950f476274bc70b4.npy\"\n",
    "load_npy_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ee3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference image support \n",
    "import numpy as np\n",
    "\n",
    "def load_image_any(path: str):\n",
    "    p = path.lower()\n",
    "    if p.endswith(\".npy\"):\n",
    "        return load_npy_image(path)\n",
    "\n",
    "    if p.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        from PIL import Image\n",
    "        x = np.array(Image.open(path).convert(\"RGB\")).astype(np.float32) / 255.0\n",
    "        return x  # (H,W,3)\n",
    "\n",
    "    if p.endswith((\".tif\", \".tiff\")):\n",
    "        import rasterio\n",
    "        with rasterio.open(path) as src:\n",
    "            x = src.read()  # (C,H,W)\n",
    "        if x.shape[0] >= 3:\n",
    "            x = x[:3]  # keep RGB if more bands\n",
    "        x = np.transpose(x, (1, 2, 0)).astype(np.float32)\n",
    "        if x.max() > 1.5:\n",
    "            x = x / 255.0 if x.max() <= 255 else x / 65535.0\n",
    "        return x\n",
    "\n",
    "    raise ValueError(f\"Unknown format: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876fe829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b06e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dec8412",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b4c2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/aminaasgarova/Desktop/dinov33\n",
      "exists dataset/train/images: True\n",
      "exists dataset/train/masks : True\n",
      "num train images: 15979\n",
      "num train masks : 15979\n",
      "first image file: /home/aminaasgarova/Desktop/dinov33/tiled/train/images/0001f0479c244ad1b0bfdd284cc0dfc0.npy\n",
      "first mask file: /home/aminaasgarova/Desktop/dinov33/tiled/train/annotations/0001f0479c244ad1b0bfdd284cc0dfc0.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"exists dataset/train/images:\", os.path.exists(\"/home/aminaasgarova/Desktop/dinov33/tiled/train/images\"))\n",
    "print(\"exists dataset/train/masks :\", os.path.exists(\"/home/aminaasgarova/Desktop/dinov33/tiled/train/annotations\"))\n",
    "\n",
    "imgs = sorted(glob(\"/home/aminaasgarova/Desktop/dinov33/tiled/train/images/*.npy\"))\n",
    "masks = sorted(glob(\"/home/aminaasgarova/Desktop/dinov33/tiled/train/annotations/*.npy\"))\n",
    "\n",
    "print(\"num train images:\", len(imgs))\n",
    "print(\"num train masks :\", len(masks))\n",
    "\n",
    "if len(imgs) > 0:\n",
    "    print(\"first image file:\", imgs[0])\n",
    "if len(masks) > 0:\n",
    "    print(\"first mask file:\", masks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SAT_MEAN = (0.430, 0.411, 0.296)\n",
    "SAT_STD  = (0.213, 0.156, 0.143)\n",
    "\n",
    "# # def normalize_img(x: torch.Tensor) -> torch.Tensor:\n",
    "#     # x: (3,H,W), float in [0,1]\n",
    "#     mean = torch.tensor(SAT_MEAN, device=x.device).view(3,1,1)  # makes a tensor from  3 numbers:,reshapes it to: (3,1,1) \n",
    "#     std  = torch.tensor(SAT_STD,  device=x.device).view(3,1,1)\n",
    "#     return (x - mean) / std\n",
    "\n",
    "SAT_MEAN_T = torch.tensor(SAT_MEAN).view(3,1,1)\n",
    "SAT_STD_T  = torch.tensor(SAT_STD).view(3,1,1)\n",
    "\n",
    "def normalize_img(x: torch.Tensor) -> torch.Tensor:\n",
    "    mean = SAT_MEAN_T.to(device=x.device, dtype=x.dtype)\n",
    "    std  = SAT_STD_T.to(device=x.device, dtype=x.dtype)\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "def to_chw_rgb(img_np: np.ndarray) -> torch.Tensor:\n",
    "    # Handles common cases:\n",
    "    # (H,W,3), (3,H,W), (H,W) grayscale\n",
    "    if img_np.ndim == 2:\n",
    "        img_np = np.stack([img_np, img_np, img_np], axis=-1)  # (H,W,3)\n",
    "    if img_np.shape[0] == 3 and img_np.ndim == 3:\n",
    "        # already (3,H,W)\n",
    "        x = torch.from_numpy(img_np)\n",
    "    else:\n",
    "        # assume (H,W,3)\n",
    "        x = torch.from_numpy(img_np).permute(2,0,1)\n",
    "    return x\n",
    "\n",
    "class NpySegDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, out_size=256, augment=False):\n",
    "        self.img_paths = sorted(glob(os.path.join(img_dir, \"*.npy\")))\n",
    "        self.mask_dir = mask_dir\n",
    "        self.out_size = out_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        mask_path = os.path.join(self.mask_dir, f\"{name}.npy\")\n",
    "\n",
    "        img_np = np.load(img_path)          # image npy\n",
    "        mask_np = np.load(mask_path)        # mask npy, values 0/1\n",
    "\n",
    "        x = to_chw_rgb(img_np).float()\n",
    "        # scale if needed\n",
    "        if x.max() > 1.0:\n",
    "            # common if uint8 [0..255]\n",
    "            x = x / 255.0\n",
    "\n",
    "        y = torch.from_numpy(mask_np).float()  # (H,W)\n",
    "        if y.ndim == 3:\n",
    "            y = y.squeeze()  # if (H,W,1)\n",
    "\n",
    "        # augment (same for x and y)\n",
    "        if self.augment:\n",
    "            if torch.rand(()) < 0.5:\n",
    "                x = torch.flip(x, dims=[2])  # horizontal flip (W)\n",
    "                y = torch.flip(y, dims=[1])\n",
    "            if torch.rand(()) < 0.5:\n",
    "                x = torch.flip(x, dims=[1])  # vertical flip (H)\n",
    "                y = torch.flip(y, dims=[0])\n",
    "\n",
    "        # resize\n",
    "        x = F.interpolate(x.unsqueeze(0), size=(self.out_size, self.out_size),\n",
    "                          mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "        y = F.interpolate(y.unsqueeze(0).unsqueeze(0), size=(self.out_size, self.out_size),\n",
    "                          mode=\"nearest\").squeeze(0).squeeze(0)\n",
    "\n",
    "        # normalize\n",
    "        x = normalize_img(x)\n",
    "\n",
    "        # shapes for training\n",
    "        y = y.unsqueeze(0)  # (1,H,W) for BCE\n",
    "        return x, y, name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251981ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleSegHead(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch=256, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, 3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(32, mid_ch)\n",
    "        self.act = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, feat, out_hw):\n",
    "        x = self.conv1(feat)\n",
    "        x = self.gn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.interpolate(x, size=out_hw, mode=\"bilinear\", align_corners=False)\n",
    "        return x\n",
    "\n",
    "class DinoV3SegModel(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # try to get embedding dim safely\n",
    "        in_dim = getattr(backbone, \"embed_dim\", None)\n",
    "        if in_dim is None:\n",
    "            in_dim = getattr(backbone, \"num_features\", None)\n",
    "        if in_dim is None:\n",
    "            raise ValueError(\"Could not find backbone embed dim. Print(backbone) and check attributes.\")\n",
    "\n",
    "        self.head = SimpleSegHead(in_ch=in_dim, out_ch=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,3,H,W)\n",
    "        B, _, H, W = x.shape\n",
    "\n",
    "        # DINO style: get patch tokens\n",
    "        # Most DINO backbones expose get_intermediate_layers.\n",
    "        # If your model errors here, print(dir(backbone)) and adapt.\n",
    "        layers = self.backbone.get_intermediate_layers(x, n=1, reshape=False)\n",
    "        tokens = layers[0]  # (B, 1+N, C) OR (B, N, C) depending on model\n",
    "\n",
    "        # handle CLS token if present\n",
    "        if tokens.dim() == 3 and tokens.shape[1] == (H // 16) * (W // 16) + 1:\n",
    "            patch_tokens = tokens[:, 1:, :]\n",
    "        else:\n",
    "            patch_tokens = tokens\n",
    "\n",
    "        # reshape tokens -> feature map\n",
    "        gh, gw = H // 16, W // 16\n",
    "        feat = patch_tokens.transpose(1, 2).contiguous().view(B, -1, gh, gw)  # (B,C,gh,gw)\n",
    "\n",
    "        logits = self.head(feat, out_hw=(H, W))  # (B,1,H,W)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022d6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_with_logits(logits, targets, eps=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    num = 2 * (probs * targets).sum(dim=(2,3))\n",
    "    den = (probs + targets).sum(dim=(2,3)) + eps\n",
    "    dice = 1 - (num / den)\n",
    "    return dice.mean()\n",
    "\n",
    "def iou_from_logits(logits, targets, thresh=0.5, eps=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > thresh).float()\n",
    "    inter = (preds * targets).sum(dim=(2,3))\n",
    "    union = (preds + targets - preds * targets).sum(dim=(2,3)) + eps\n",
    "    return (inter / union).mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4c0e858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1056450/1277804698.py:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
      "/tmp/ipykernel_1056450/1277804698.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss 0.4245 | val_loss 0.3713 | val_iou 0.5594\n",
      "Saved best model\n",
      "Epoch 02 | train_loss 0.4004 | val_loss 0.3774 | val_iou 0.5495\n",
      "Epoch 03 | train_loss 0.3930 | val_loss 0.3573 | val_iou 0.5732\n",
      "Saved best model\n",
      "Epoch 04 | train_loss 0.3871 | val_loss 0.3534 | val_iou 0.5859\n",
      "Saved best model\n",
      "Epoch 05 | train_loss 0.3830 | val_loss 0.3462 | val_iou 0.5946\n",
      "Saved best model\n",
      "Epoch 06 | train_loss 0.3801 | val_loss 0.3477 | val_iou 0.5926\n",
      "Epoch 07 | train_loss 0.3775 | val_loss 0.3379 | val_iou 0.5888\n",
      "Epoch 08 | train_loss 0.3747 | val_loss 0.3340 | val_iou 0.5949\n",
      "Saved best model\n",
      "Epoch 09 | train_loss 0.3725 | val_loss 0.3368 | val_iou 0.5962\n",
      "Saved best model\n",
      "Epoch 10 | train_loss 0.3709 | val_loss 0.3360 | val_iou 0.6028\n",
      "Saved best model\n",
      "Epoch 11 | train_loss 0.3687 | val_loss 0.3323 | val_iou 0.6051\n",
      "Saved best model\n",
      "Epoch 12 | train_loss 0.3671 | val_loss 0.3316 | val_iou 0.6029\n",
      "Epoch 13 | train_loss 0.3660 | val_loss 0.3274 | val_iou 0.6026\n",
      "Epoch 14 | train_loss 0.3643 | val_loss 0.3298 | val_iou 0.6104\n",
      "Saved best model\n",
      "Epoch 15 | train_loss 0.3633 | val_loss 0.3279 | val_iou 0.6056\n",
      "Epoch 16 | train_loss 0.3620 | val_loss 0.3309 | val_iou 0.6100\n",
      "Epoch 17 | train_loss 0.3611 | val_loss 0.3312 | val_iou 0.6091\n",
      "Epoch 18 | train_loss 0.3594 | val_loss 0.3272 | val_iou 0.6104\n",
      "Epoch 19 | train_loss 0.3592 | val_loss 0.3283 | val_iou 0.6131\n",
      "Saved best model\n",
      "Epoch 20 | train_loss 0.3573 | val_loss 0.3304 | val_iou 0.6077\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) load backbone (your code)\n",
    "DINOv3_REPO = \"/home/aminaasgarova/Desktop/dinov3/dinov3\"\n",
    "DINOv3_weights = \"/home/aminaasgarova/Desktop/dinov3/dinov3/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\"\n",
    "dinov3_vitl16 = torch.hub.load(DINOv3_REPO, 'dinov3_vitl16', source='local', weights = DINOv3_weights)\n",
    "\n",
    "backbone = dinov3_vitl16.to(device)\n",
    "\n",
    "# 2) build model\n",
    "model = DinoV3SegModel(backbone=backbone, num_classes=1).to(device)\n",
    "\n",
    "# 3) freeze backbone first\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 4) data\n",
    "train_ds = NpySegDataset(\n",
    "    img_dir=\"/home/aminaasgarova/Desktop/dinov33/tiled/train/images\",\n",
    "    mask_dir=\"/home/aminaasgarova/Desktop/dinov33/tiled/train/annotations\",\n",
    "    out_size=256,\n",
    "    augment=True,\n",
    ")\n",
    "val_ds = NpySegDataset(\n",
    "    img_dir = \"/home/aminaasgarova/Desktop/dinov33/tiled/val/images\",\n",
    "    mask_dir=\"/home/aminaasgarova/Desktop/dinov33/tiled/val/annotations\",\n",
    "    out_size=256,\n",
    "    augment=False,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 5) optimizer\n",
    "optimizer = torch.optim.AdamW(model.head.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "best_iou = -1.0\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for x, y, _ in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "            logits = model(x)\n",
    "            loss = bce(logits, y) + 0.5 * dice_loss_with_logits(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_iou = 0.0\n",
    "    val_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in val_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = bce(logits, y) + 0.5 * dice_loss_with_logits(logits, y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_iou += iou_from_logits(logits, y)\n",
    "            n += 1\n",
    "\n",
    "    train_loss /= max(1, len(train_loader))\n",
    "    val_loss /= max(1, n)\n",
    "    val_iou /= max(1, n)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | val_iou {val_iou:.4f}\")\n",
    "\n",
    "    if val_iou > best_iou:\n",
    "        best_iou = val_iou\n",
    "        torch.save(\n",
    "            {\"model\": model.state_dict(), \"epoch\": epoch, \"best_iou\": best_iou},\n",
    "            \"best_building_seg.pth\",\n",
    "        )\n",
    "        print(\"Saved best model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze backbone\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# smaller lr for backbone\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": model.backbone.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": model.head.parameters(), \"lr\": 1e-4},\n",
    "    ],\n",
    "    weight_decay=1e-2,\n",
    ")\n",
    "# freeze some layers of backbone used used in model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fe3a74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pred01, prob.cpu().numpy()\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# usage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mmodel\u001b[49m.to(device).eval()\n\u001b[32m     72\u001b[39m pred01, prob = infer_geotiff(model, \u001b[33m\"\u001b[39m\u001b[33m/home/aminaasgarova/Desktop/dinov33/test_data/test_image.tif\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m/home/aminaasgarova/Desktop/dinov33/test_data/pred_mask.tif\u001b[39m\u001b[33m\"\u001b[39m, device, rgb_bands=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import rasterio\n",
    "\n",
    "SAT_MEAN = (0.430, 0.411, 0.296)\n",
    "SAT_STD  = (0.213, 0.156, 0.143)\n",
    "\n",
    "def normalize_img_3ch(x: torch.Tensor) -> torch.Tensor:\n",
    "    mean = torch.tensor(SAT_MEAN, device=x.device).view(3,1,1)\n",
    "    std  = torch.tensor(SAT_STD,  device=x.device).view(3,1,1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "def pad_to_multiple(x: torch.Tensor, mult: int = 16):\n",
    "    _, _, H, W = x.shape\n",
    "    pad_h = (mult - H % mult) % mult\n",
    "    pad_w = (mult - W % mult) % mult\n",
    "    x_pad = F.pad(x, (0, pad_w, 0, pad_h), mode=\"reflect\")\n",
    "    return x_pad, H, W\n",
    "\n",
    "def scale_to_01(arr: np.ndarray) -> np.ndarray:\n",
    "    # arr: float32 or int, shape (C,H,W)\n",
    "    # simple robust scaling per band\n",
    "    arr = arr.astype(np.float32)\n",
    "    out = np.empty_like(arr, dtype=np.float32)\n",
    "    for c in range(arr.shape[0]):\n",
    "        band = arr[c]\n",
    "        lo, hi = np.percentile(band, 2), np.percentile(band, 98)\n",
    "        if hi <= lo:\n",
    "            out[c] = 0.0\n",
    "        else:\n",
    "            out[c] = np.clip((band - lo) / (hi - lo), 0.0, 1.0)\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_geotiff(model, tif_path, out_mask_tif, device, out_thresh=0.5, rgb_bands=(1,2,3)):\n",
    "    # rgb_bands are 1-based indices in rasterio\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        meta = src.meta.copy()\n",
    "        # read selected bands\n",
    "        arr = src.read(list(rgb_bands))  # (3,H,W) usually\n",
    "        H0, W0 = arr.shape[1], arr.shape[2]\n",
    "\n",
    "    # Convert to float [0,1]\n",
    "    # If it is already uint8 RGB, scaling by 255 works.\n",
    "    # If it is uint16 or reflectance, use robust scaling.\n",
    "    if arr.dtype == np.uint8:\n",
    "        arr01 = arr.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        arr01 = scale_to_01(arr)\n",
    "\n",
    "    x = torch.from_numpy(arr01).to(device)           # (3,H,W)\n",
    "    x = x.unsqueeze(0)                               # (1,3,H,W)\n",
    "    x, Horig, Worig = pad_to_multiple(x, 16)         # (1,3,Hpad,Wpad)\n",
    "    x = normalize_img_3ch(x[0]).unsqueeze(0)\n",
    "\n",
    "    logits = model(x)\n",
    "    prob = torch.sigmoid(logits)[0,0]                # (Hpad,Wpad)\n",
    "    prob = prob[:Horig, :Worig]\n",
    "\n",
    "    pred01 = (prob > out_thresh).to(torch.uint8).cpu().numpy()  # (H,W) 0/1\n",
    "\n",
    "    # save as GeoTIFF mask with same georef\n",
    "    meta.update(count=1, dtype=\"uint8\")\n",
    "    with rasterio.open(out_mask_tif, \"w\", **meta) as dst:\n",
    "        dst.write((pred01 * 255).astype(np.uint8), 1)\n",
    "\n",
    "    return pred01, prob.cpu().numpy()\n",
    "\n",
    "# usage\n",
    "model.to(device).eval()\n",
    "pred01, prob = infer_geotiff(model, \"/home/aminaasgarova/Desktop/dinov33/test_data/test_image.tif\", \"/home/aminaasgarova/Desktop/dinov33/test_data/pred_mask.tif\", device, rgb_bands=(1,2,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape\n",
    "import geopandas as gpd\n",
    "\n",
    "def pred_tif_to_vector(pred_tif_path, out_path, min_area=None):\n",
    "    with rasterio.open(pred_tif_path) as src:\n",
    "        mask = src.read(1)          # (H,W)\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "\n",
    "    # Make it binary (works for 0/1 or 0/255 or any >0)\n",
    "    bin_mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "    geoms = []\n",
    "    for geom, val in shapes(bin_mask, mask=(bin_mask > 0), transform=transform):\n",
    "        if val == 1:\n",
    "            geoms.append(shape(geom))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(geometry=geoms, crs=crs)\n",
    "\n",
    "    if len(gdf) == 0:\n",
    "        print(\"No polygons extracted. Check mask values and threshold.\")\n",
    "    else:\n",
    "        # optional: remove tiny polygons\n",
    "        if min_area is not None:\n",
    "            gdf = gdf[gdf.geometry.area >= float(min_area)]\n",
    "\n",
    "    if out_path.lower().endswith(\".shp\"):\n",
    "        gdf.to_file(out_path, driver=\"ESRI Shapefile\")\n",
    "    else:\n",
    "        gdf.to_file(out_path, driver=\"GeoJSON\")\n",
    "\n",
    "    print(\"polygons:\", len(gdf))\n",
    "    return gdf\n",
    "\n",
    "# example\n",
    "pred_tif_to_vector(\n",
    "    pred_tif_path=\"/home/aminaasgarova/Desktop/dinov33/test_data/pred_mask.tif\",\n",
    "    out_path=\"/home/aminaasgarova/Desktop/dinov33/test_data/buildings.geojson\",   # or \"buildings.shp\"\n",
    "    min_area=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d17e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b936f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6297b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f257a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f98d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "019e4c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aminaasgarova/miniconda3/envs/dinov33/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch: 19 best_iou: 0.6131070247718267\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DINOv3_REPO = \"/home/aminaasgarova/Desktop/dinov3/dinov3\"\n",
    "DINOv3_weights = \"/home/aminaasgarova/Desktop/dinov3/dinov3/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\"\n",
    "CKPT_PATH = \"/home/aminaasgarova/Desktop/dinov33/best_building_seg.pth\"  # change if you saved elsewhere\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleSegHead(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch=256, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, 3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(32, mid_ch)\n",
    "        self.act = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, feat, out_hw):\n",
    "        x = self.conv1(feat)\n",
    "        x = self.gn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.interpolate(x, size=out_hw, mode=\"bilinear\", align_corners=False)\n",
    "        return x\n",
    "\n",
    "class DinoV3SegModel(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        in_dim = getattr(backbone, \"embed_dim\", None)\n",
    "        if in_dim is None:\n",
    "            in_dim = getattr(backbone, \"num_features\", None)\n",
    "        if in_dim is None:\n",
    "            raise ValueError(\"Could not find backbone embed dim.\")\n",
    "\n",
    "        self.head = SimpleSegHead(in_ch=in_dim, out_ch=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.shape\n",
    "        layers = self.backbone.get_intermediate_layers(x, n=1, reshape=False)\n",
    "        tokens = layers[0]\n",
    "\n",
    "        if tokens.shape[1] == (H // 16) * (W // 16) + 1:\n",
    "            patch_tokens = tokens[:, 1:, :]\n",
    "        else:\n",
    "            patch_tokens = tokens\n",
    "\n",
    "        gh, gw = H // 16, W // 16\n",
    "        feat = patch_tokens.transpose(1, 2).contiguous().view(B, -1, gh, gw)\n",
    "        logits = self.head(feat, out_hw=(H, W))\n",
    "        return logits\n",
    "\n",
    "# 1) load backbone\n",
    "backbone = torch.hub.load(DINOv3_REPO, \"dinov3_vitl16\", source=\"local\", weights=DINOv3_weights).to(device)\n",
    "\n",
    "# 2) build model (must match training)\n",
    "model = DinoV3SegModel(backbone=backbone, num_classes=1).to(device)\n",
    "\n",
    "# 3) load checkpoint\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded checkpoint from epoch:\", ckpt.get(\"epoch\"), \"best_iou:\", ckpt.get(\"best_iou\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab95323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "SAT_MEAN = (0.430, 0.411, 0.296)\n",
    "SAT_STD  = (0.213, 0.156, 0.143)\n",
    "\n",
    "def normalize_img_3ch(x: torch.Tensor) -> torch.Tensor:\n",
    "    mean = torch.tensor(SAT_MEAN, device=x.device).view(3,1,1)\n",
    "    std  = torch.tensor(SAT_STD,  device=x.device).view(3,1,1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "def pad_to_multiple(x: torch.Tensor, mult: int = 16):\n",
    "    _, _, H, W = x.shape\n",
    "    pad_h = (mult - H % mult) % mult\n",
    "    pad_w = (mult - W % mult) % mult\n",
    "    x_pad = F.pad(x, (0, pad_w, 0, pad_h), mode=\"reflect\")\n",
    "    return x_pad, H, W\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_geotiff_save_tif(model, tif_path, out_tif, device, thresh=0.5, rgb_bands=(1,2,3)):\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        meta = src.meta.copy()\n",
    "        arr = src.read(list(rgb_bands))  # (3,H,W)\n",
    "        H0, W0 = arr.shape[1], arr.shape[2]\n",
    "\n",
    "    # scale to 0..1\n",
    "    if arr.dtype == np.uint8:\n",
    "        arr01 = arr.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        arr01 = arr.astype(np.float32)\n",
    "        # quick robust scaling per band\n",
    "        for c in range(3):\n",
    "            lo, hi = np.percentile(arr01[c], 2), np.percentile(arr01[c], 98)\n",
    "            arr01[c] = 0.0 if hi <= lo else np.clip((arr01[c] - lo) / (hi - lo), 0, 1)\n",
    "\n",
    "    x = torch.from_numpy(arr01).to(device).unsqueeze(0)\n",
    "    x, Horig, Worig = pad_to_multiple(x, 16)\n",
    "    x = normalize_img_3ch(x[0]).unsqueeze(0)\n",
    "\n",
    "    logits = model(x)\n",
    "    prob = torch.sigmoid(logits)[0,0][:Horig, :Worig]\n",
    "    pred01 = (prob > thresh).to(torch.uint8).cpu().numpy()\n",
    "\n",
    "    meta.update(count=1, dtype=\"uint8\")\n",
    "    with rasterio.open(out_tif, \"w\", **meta) as dst:\n",
    "        dst.write((pred01 * 255).astype(np.uint8), 1)\n",
    "\n",
    "    return pred01\n",
    "\n",
    "# example\n",
    "pred01 = infer_geotiff_save_tif(model, \"/home/aminaasgarova/Desktop/dinov33/test_data/test2.tif\", \"/home/aminaasgarova/Desktop/dinov33/test_data/pred_mask2.tif\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763c1833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polygons: 199\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((536509.25 4341833.75, 536509.25 4341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLYGON ((536571.75 4341833.75, 536571.75 4341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POLYGON ((536513.75 4341829, 536513.75 4341828...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLYGON ((536761.5 4341833.75, 536761.5 434183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLYGON ((536605 4341831.25, 536605 4341831, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>POLYGON ((536393.75 4341588.5, 536393.75 43415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>POLYGON ((536573.25 4341589, 536573.25 4341588...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>POLYGON ((536589.5 4341589.75, 536589.5 434158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>POLYGON ((536533.5 4341592.75, 536533.5 434159...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>POLYGON ((536521.75 4341591.75, 536521.75 4341...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              geometry\n",
       "0    POLYGON ((536509.25 4341833.75, 536509.25 4341...\n",
       "1    POLYGON ((536571.75 4341833.75, 536571.75 4341...\n",
       "2    POLYGON ((536513.75 4341829, 536513.75 4341828...\n",
       "3    POLYGON ((536761.5 4341833.75, 536761.5 434183...\n",
       "4    POLYGON ((536605 4341831.25, 536605 4341831, 5...\n",
       "..                                                 ...\n",
       "194  POLYGON ((536393.75 4341588.5, 536393.75 43415...\n",
       "195  POLYGON ((536573.25 4341589, 536573.25 4341588...\n",
       "196  POLYGON ((536589.5 4341589.75, 536589.5 434158...\n",
       "197  POLYGON ((536533.5 4341592.75, 536533.5 434159...\n",
       "198  POLYGON ((536521.75 4341591.75, 536521.75 4341...\n",
       "\n",
       "[199 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape\n",
    "import geopandas as gpd\n",
    "\n",
    "def pred_tif_to_vector(pred_tif_path, out_path):\n",
    "    with rasterio.open(pred_tif_path) as src:\n",
    "        mask = src.read(1)\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "\n",
    "    bin_mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "    geoms = []\n",
    "    for geom, val in shapes(bin_mask, mask=(bin_mask > 0), transform=transform):\n",
    "        if val == 1:\n",
    "            geoms.append(shape(geom))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(geometry=geoms, crs=crs)\n",
    "\n",
    "    if out_path.lower().endswith(\".shp\"):\n",
    "        gdf.to_file(out_path, driver=\"ESRI Shapefile\")\n",
    "    else:\n",
    "        gdf.to_file(out_path, driver=\"GeoJSON\")\n",
    "\n",
    "    print(\"polygons:\", len(gdf))\n",
    "    return gdf\n",
    "\n",
    "pred_tif_to_vector(\"/home/aminaasgarova/Desktop/dinov33/test_data/pred_mask2.tif\", \"/home/aminaasgarova/Desktop/dinov33/test_data/buildings2.geojson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37265f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cb01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e59a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21c4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a899a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebc01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643e3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd6a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c374a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d52dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c882298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a75f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc5b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f831b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf171a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aminaasgarova/miniconda3/envs/dinov33/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 768, 32, 32]' is invalid for input of size 3161088",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 212\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# #  inference \u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# def inference():\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m#     backbone = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    209\u001b[39m \n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# main \u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     \u001b[38;5;66;03m# inference()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 161\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m imgs, masks \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m    160\u001b[39m     imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     loss = criterion(preds, masks)\n\u001b[32m    164\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov33/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dinov33/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mDinoSeg.forward\u001b[39m\u001b[34m(self, images)\u001b[39m\n\u001b[32m    119\u001b[39m b, n, c = out.shape\n\u001b[32m    120\u001b[39m h = w = \u001b[38;5;28mint\u001b[39m(n ** \u001b[32m0.5\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m feats = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.head(feats, images.shape[-\u001b[32m2\u001b[39m:])\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[4, 768, 32, 32]' is invalid for input of size 3161088"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel\n",
    "\n",
    "# config\n",
    "TRAIN_IMG = \"/home/aminaasgarova/Desktop/dinov33/tiled/train/images\"\n",
    "TRAIN_MASK = \"/home/aminaasgarova/Desktop/dinov33/tiled/train/annotations\"\n",
    "VAL_IMG = \"/home/aminaasgarova/Desktop/dinov33/tiled/val/images\"\n",
    "VAL_MASK = \"/home/aminaasgarova/Desktop/dinov33/tiled/val/annotations\"\n",
    "# TEST_IMG = \"dataset/test/images\"\n",
    "# SAVE_PRED = \"predictions\"\n",
    "\n",
    "MODEL_NAME = \"facebook/dinov3-vitb16-pretrain-lvd1689m\"\n",
    "IMG_SIZE = 512\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# os.makedirs(SAVE_PRED, exist_ok=True)\n",
    "\n",
    "#dataset\n",
    "class NpyDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = sorted(os.listdir(img_dir))\n",
    "\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.files[idx]\n",
    "\n",
    "        img = np.load(os.path.join(self.img_dir, name))\n",
    "        img = torch.from_numpy(img).float()\n",
    "\n",
    "        # ---- handle channel order ----\n",
    "        if img.ndim != 3:\n",
    "            raise ValueError(f\"Bad image ndim {img.shape}\")\n",
    "\n",
    "        # H W 3 -> 3 H W\n",
    "        if img.shape[-1] == 3:\n",
    "            img = img.permute(2, 0, 1)\n",
    "\n",
    "        # already 3 H W\n",
    "        elif img.shape[0] == 3:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image shape {img.shape}\")\n",
    "\n",
    "        # normalize range\n",
    "        if img.max() > 1:\n",
    "            img = img / 255.0\n",
    "\n",
    "        # resize\n",
    "        img = F.interpolate(\n",
    "            img.unsqueeze(0),\n",
    "            size=(IMG_SIZE, IMG_SIZE),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # imagenet normalization\n",
    "        img = (img - self.mean) / self.std\n",
    "\n",
    "        # inference only\n",
    "        if self.mask_dir is None:\n",
    "            return img, name\n",
    "\n",
    "        # ---- mask ----\n",
    "        mask = np.load(os.path.join(self.mask_dir, name))\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        if mask.ndim != 2:\n",
    "            raise ValueError(f\"Bad mask shape {mask.shape}\")\n",
    "\n",
    "        mask = F.interpolate(\n",
    "            mask.unsqueeze(0).unsqueeze(0).float(),\n",
    "            size=(IMG_SIZE, IMG_SIZE),\n",
    "            mode=\"nearest\",\n",
    "        ).squeeze(0).long()\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# model \n",
    "class SegHead(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, 256, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(256, num_classes, 1)\n",
    "\n",
    "    def forward(self, x, out_size):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return F.interpolate(x, size=out_size, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "class DinoSeg(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(images).last_hidden_state\n",
    "\n",
    "        b, n, c = out.shape\n",
    "        h = w = int(n ** 0.5)\n",
    "        feats = out.permute(0, 2, 1).reshape(b, c, h, w)\n",
    "\n",
    "        return self.head(feats, images.shape[-2:])\n",
    "\n",
    "# metrics \n",
    "def dice(pred, gt):\n",
    "    pred = pred.flatten()\n",
    "    gt = gt.flatten()\n",
    "    inter = (pred * gt).sum()\n",
    "    return (2 * inter + 1e-6) / (pred.sum() + gt.sum() + 1e-6)\n",
    "\n",
    "def iou(pred, gt):\n",
    "    inter = (pred & gt).sum()\n",
    "    union = (pred | gt).sum()\n",
    "    return (inter + 1e-6) / (union + 1e-6)\n",
    "\n",
    "# training \n",
    "def train():\n",
    "    backbone = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    for p in backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "    backbone.eval()\n",
    "\n",
    "    model = DinoSeg(backbone, SegHead(768, NUM_CLASSES)).to(DEVICE)\n",
    "\n",
    "    train_ds = NpyDataset(TRAIN_IMG, TRAIN_MASK)\n",
    "    val_ds = NpyDataset(VAL_IMG, VAL_MASK)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, BATCH_SIZE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.head.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        loss_sum = 0\n",
    "\n",
    "        for imgs, masks in train_loader:\n",
    "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        dices, ious = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "                pred = model(imgs).argmax(1)\n",
    "                dices.append(dice(pred, masks).item())\n",
    "                ious.append(iou(pred.bool(), masks.bool()).item())\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} | \"\n",
    "            f\"Loss {loss_sum/len(train_loader):.4f} | \"\n",
    "            f\"Dice {np.mean(dices):.4f} | \"\n",
    "            f\"IoU {np.mean(ious):.4f}\"\n",
    "        )\n",
    "\n",
    "    torch.save(model.head.state_dict(), \"seg_head.pth\")\n",
    "\n",
    "# #  inference \n",
    "# def inference():\n",
    "#     backbone = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "#     backbone.eval()\n",
    "\n",
    "#     head = SegHead(768, NUM_CLASSES).to(DEVICE)\n",
    "#     head.load_state_dict(torch.load(\"seg_head.pth\"))\n",
    "#     model = DinoSeg(backbone, head).to(DEVICE)\n",
    "#     model.eval()\n",
    "\n",
    "#     test_ds = NpyDataset(TEST_IMG)\n",
    "#     loader = DataLoader(test_ds, batch_size=1)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for img, name in loader:\n",
    "#             img = img.to(DEVICE)\n",
    "#             pred = model(img).argmax(1).squeeze(0).cpu().numpy()\n",
    "#             np.save(os.path.join(SAVE_PRED, name[0]), pred)\n",
    "\n",
    "#     print(\"Inference done\")\n",
    "\n",
    "# main \n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    # inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f028911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov33",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
